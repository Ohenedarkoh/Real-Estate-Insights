{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27cce28-0687-420f-85f5-e5f781488c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0: https://meqasa.com/properties-for-rent-in-ghana?w=0\n",
      "Found 0 listings on page 0\n",
      "Scraping page 1: https://meqasa.com/properties-for-rent-in-ghana?w=1\n",
      "Found 0 listings on page 1\n",
      "Scraping page 2: https://meqasa.com/properties-for-rent-in-ghana?w=2\n",
      "Found 16 listings on page 2\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"scapemeqasa\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1xFtyhu5u4K25GWckn3_Opig3cnTGqk0T\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class MeqasaScraper:\n",
    "    def __init__(self, base_url, headers):\n",
    "        self.base_url = base_url\n",
    "        self.headers = headers\n",
    "        self.all_data = []\n",
    "\n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Fetch the HTML content of a page.\"\"\"\n",
    "        response = requests.get(url, headers=self.headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to retrieve page: {url}. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def parse_listing_page(self, html):\n",
    "        \"\"\"Parse the HTML content of a listing page and extract property details.\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        listings = soup.find_all(\"div\", class_=\"mqs-prop-dt-wrapper\")\n",
    "        return listings\n",
    "\n",
    "    def extract_listing_details(self, listing):\n",
    "        \"\"\"Extract details from a single listing.\"\"\"\n",
    "        title = listing.find(\"h2\")\n",
    "        title = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        price = listing.find(\"p\", class_=\"h3\")\n",
    "        price = price.text.strip().replace(\"Price:\", \"\").strip() if price else \"Price not found\"\n",
    "\n",
    "        listing_url = listing.find(\"a\", href=True)\n",
    "        listing_url = \"https://meqasa.com\" + listing_url[\"href\"] if listing_url else \"URL not found\"\n",
    "\n",
    "        return {\n",
    "            \"Title\": title,\n",
    "            \"Price\": price,\n",
    "            \"URL\": listing_url,\n",
    "        }\n",
    "\n",
    "    def scrape_listing_details(self, listing_url):\n",
    "        \"\"\"Scrape additional details from an individual listing page.\"\"\"\n",
    "        html = self.fetch_page(listing_url)\n",
    "        if not html:\n",
    "            return {}\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        details = {}\n",
    "\n",
    "        table = soup.find(\"table\", class_=\"table table-hover table-bordered\")\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")\n",
    "\n",
    "            for row in rows:\n",
    "                header = row.find(\"td\", style=\"font-weight: bold;\") or row.find(\"th\")\n",
    "                if header:\n",
    "                    header_text = header.text.strip()\n",
    "                    data_cell = header.find_next(\"td\")\n",
    "\n",
    "                    if data_cell:\n",
    "                        data_text = data_cell.text.strip()\n",
    "\n",
    "                        if header_text == \"Commission\":\n",
    "                            details[\"Commission\"] = data_text\n",
    "                        elif header_text == \"Bedrooms\":\n",
    "                            details[\"Bedrooms\"] = data_text\n",
    "                        elif header_text == \"Bathrooms\":\n",
    "                            details[\"Bathrooms\"] = data_text\n",
    "                        elif header_text == \"Garage\":\n",
    "                            details[\"Garage\"] = data_text\n",
    "                        elif header_text == \"Furnished\":\n",
    "                            details[\"Furnished\"] = data_text\n",
    "                        elif header_text == \"Area\":\n",
    "                            details[\"Area\"] = data_text\n",
    "                        elif header_text == \"Amenities\":\n",
    "                            amenities = [li.text.strip() for li in data_cell.find_all(\"li\")]\n",
    "                            details[\"Amenities\"] = \", \".join(amenities)\n",
    "                        elif header_text == \"Address\":\n",
    "                            details[\"Address\"] = data_text\n",
    "                        elif header_text == \"Reference\":\n",
    "                            details[\"Reference\"] = data_text\n",
    "\n",
    "        description_div = soup.find(\"div\", class_=\"description\")\n",
    "        if description_div:\n",
    "            description = description_div.find(\"p\").text.strip()\n",
    "            details[\"Description\"] = description\n",
    "        else:\n",
    "            details[\"Description\"] = \"Description not found\"\n",
    "\n",
    "        return details\n",
    "\n",
    "    def scrape_page(self, page):\n",
    "        \"\"\"Scrape a single page of listings.\"\"\"\n",
    "        url = f\"{self.base_url}?w={page}\"\n",
    "        print(f\"Scraping page {page}: {url}\")\n",
    "\n",
    "        html = self.fetch_page(url)\n",
    "        if not html:\n",
    "            return\n",
    "\n",
    "        listings = self.parse_listing_page(html)\n",
    "        print(f\"Found {len(listings)} listings on page {page}\")\n",
    "\n",
    "        for listing in listings:\n",
    "            listing_data = self.extract_listing_details(listing)\n",
    "\n",
    "            if listing_data[\"URL\"] != \"URL not found\":\n",
    "                additional_details = self.scrape_listing_details(listing_data[\"URL\"])\n",
    "            else:\n",
    "                additional_details = {}\n",
    "\n",
    "            # Combine the data\n",
    "            listing_data.update(additional_details)\n",
    "            self.all_data.append(listing_data)\n",
    "\n",
    "    def scrape_all_pages(self, start_page=1, end_page=5):\n",
    "        \"\"\"Scrape all pages from start_page to end_page.\"\"\"\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            self.scrape_page(page)\n",
    "            time.sleep(2)\n",
    "\n",
    "    def save_to_csv(self, filename=\"raw.csv\"):\n",
    "        \"\"\"Save the scraped data to a CSV file.\"\"\"\n",
    "        if self.all_data:\n",
    "            df = pd.DataFrame(self.all_data)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Data saved to {filename}\")\n",
    "        else:\n",
    "            print(\"No data found. CSV file not created.\")\n",
    "\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://meqasa.com/properties-for-rent-in-ghana\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    scraper = MeqasaScraper(base_url, headers)\n",
    "\n",
    "    scraper.scrape_all_pages(start_page=0, end_page=870)\n",
    "\n",
    "    scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e5cf3-7177-4f49-9ee0-90822476508a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
